# profitable_ai

書籍: Python で儲かる AI をつくる の学習用 Repository

# 機械学習 Model 開発の重要 Points

| Model の開発 Flow  | Model の重要 Points |
|-----------------|------------------|
| 1. Data の読み込み   |                  |
| 2. Data の確認     | *1               |
| 3. Data 前処理     | *2               |
| 4. Data 分割      |                  |
| 5. Algorithm 選択 | ***              |
| 6. 学習           |                  |
| 7. 予測           |                  |
| 8. 評価           | ***              |
| 9. Tuning       | ***              |

# *1) Data 確認

精度の良い Model をつくる為には、学習 Data の状況を正確に確認することが必須。

## 確認方法

1. DataFrame の機能を使用して**数値的・統計的に分析する方法**
2. MatplotLib や Seaborn の Graph 描画機能を使って**視覚的に分析・確認する方法**

### 数値的・統計的に分析する方法

1. Data 欠損値の調査  
   最初の重要な Task。現実世界では Data がそろっていないことの方が多い。機械学習 Model を作るにあたって、欠損値の存在は精度低下の大きな理由となる。  
   そのため、事前にどの程度欠損値があるかを調査することが重要。
2. 項目値（Label 値）の個数 Count  
   項目の値が数値ではなく、どの Group に属しているかを示す値（項目値、Label 値）の場合は、「それぞれの値が何個あるか」を事前に調べておくべき。
3. 統計情報の調査  
   DataFrame の `describe()`関数で
    - 平均
    - 分散
    - 個数
    - 最大値
    - 最小値

   などの色々な統計情報をまとめて調べられる。（※ 数値を値として持つ項目のみが処理対象）
4. 集約関数の利用  
   特定の項目で Group 分けそして、その Group に関して集計を行なう。
5. Graph 表示  
   分析結果を Graph 化し分布状況などを視覚的に確認する。

### *2) Data 前処理

**同じ情報量の Data 項目が複数あると、精度はかえって落ちる。**為、同じことを意味するような項目は削除する。

#### 残す項目の基準

1. 順序関係がある項目は数値で表している項目があれば数値を優先。
2. Boolean のような値は 0/1 で表現しているものがあれば、そちら。最終的に 0/1 に変換することになる。

上記以外の場合は、実装にしやすさと人間のわかりやすさを考慮して決定。

## One-Hot encoding

種類が３種類以上ある、多項 Label を数値化するための方法。

多項 Label は考え方・支店で順番が変わってしまい、いつでも通用する数字は決められない為、**それぞれの Label に対応した列を新しく作り（※項目数（次元数）、 該当する項目は値１を、関係ない項目には０を割り当てる**。

### One-Hot encoding の注意点

機会学習 Model では、

- 入力の Data 項目数があまり多すぎるとうまくいかない傾向がある。
- 学習がうまくいくにしても、項目数が増えることでより多くの学習 Data が必要になる（ = 機会学習の最重要で Cost のかかる「学習 Data 収集」の負荷が高くなってしまう。）

機械学習 Model を実装する上でうまくいくかどうかを決める重要な要素になり、業務専門家も意識する必要がある。

# 正規化

機会学習では、学習用の Data が絶対値が１前後の数値（おおよそ-1 ~ 1 の範囲の値）が精度が高くなることがわかっている。  
<small>※ Algorithm によって関係ないものもある。決定木などは不要。</small>

上記を踏まえて、学習直前の段階で、Model の入力 Data を変換する操作のことを**正規化**という。

## 正規化の手法

### normalization

入力変数 x を、最小値が０、最大値が１になるように１次関数を使用して変換する。  
<small>

- 外れ値の影響を受けやすい。
- 画像 Data のように最大値と最小値が事前にわかっている場合は利用しやすい。
  </small>

### standardization

入力変数 x が、平均０、分散１の正規分布になるように、１次関数を使って変換する。  
対象項目が外れ値を含んでいる可能性がある場合に利用すると無難。

## 特微量 Engineering

Data 前処理において

- 離散化
- 対数をとった加工
- 三角関数を利用した加工

など状況に応じて複数の入力 Data を組み合わせて新しい入力 Data を作り、精度の良い Model をつくる為に行なう高度は前処理のこと。  
DataScientist の Task のうち、最も高度な Task と言われている。

### 離散化

年齢などの数値 Data を「10代」「20代」のような、Label 値に分類すること。

### 対数をとる

企業のの年間売上のように、企業の規模により様々な値を取り得る数値を対数をとって表す。

### 三角関数

周期的に変化することがわかっている数値に対して有効な場合がある。

# Algorithm 選択

## 分類の代表的な Algorithm とその特徴
表形式の「構造化 Data」が対象であれば、下記表の Model でほぼ Cover できる。

| Algorithm 名                     | 実装方式  | 特徴                                       |
|---------------------------------|-------|------------------------------------------|
| Logistic 回帰                     | 損失関数型 | 「Sigmoid 関数」の出力を確率とみなす。境界は直線             |
| Support vector machine(Kernael) | 損失関数型 | Kernel trick という方法で、直線以外の境界を実現           |
| Neutral network                 | 損失関数型 | 隠れ層を追加することで直線以外の境界を実現                    |
| 決定木                             | 決定木型  | 特定の項目値を基準にした Group 分けを複数回実施              |
| Random forest                   | 決定木型  | 学習 Data の Subset から複数の決定木を作り、多数決で決定      |
| XGBoost                         | 決定木型  | 分類がうまくいかなかった Data から分類する Model を作り、精度を向上 |

### 実装方式

Data の正規化が損失関数型の Algorithm では有効だが、決定木型では不要など実装方式によって必要な前処理に違いがでてくる。

#### 損失関数型

Model の構造が数学的な関数になっており、Parameters を最適化することが学習になる。  
Model の精度が良いほど値が小さくなる（全て正解だと０になる）「損失関数」を定義して数学的手法でこの値が小さくなるような Parameters 値を求める。

入力変数の値が極端に大きかったり、小さかったりすると Algorithm がうまく働かないことがあるため、Data 前処理で正規化を行なう方が良い。

#### 決定木型

特定の項目に閾値を定め、その値よりも大きいか小さいかで Group 分けをする方式。  
どの項目値をどの閾値で Group 分けするか、その Rule を決めていくのが学習になる。

- Random forest
- XGBooost

の２つは決定木の改良版で、複数の決定木を組み合わせて Model の精度を向上させる。

単に値の大小だけで分類していくので、正規化のような Scale の調整は不要。

##### その他の Algorithm

| Algorithm | 概要                                                     |
|-----------|--------------------------------------------------------|
| 単純 bayes  | Text 系の Data 分類に利用されることが多い。                            |
| k 近傍法     | 古くからある Algorithm で「近くの１点を調べ、１番多い Class に自分を分類する」という方式。 |

